{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lean GRPO Tutorial\n",
    "\n",
    "This notebook demonstrates how to use Lean GRPO to train LLMs for Lean 4 proof generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "\n",
    "from lean_grpo import (\n",
    "    LeanGRPOConfig,\n",
    "    LeanGRPOTrainer,\n",
    "    LeanInterface,\n",
    "    MockLeanInterface,\n",
    "    MockInferenceClient,\n",
    "    ProofRolloutGenerator,\n",
    "    VLLMClient,\n",
    "    REWARD_CONFIG_SHAPED,\n",
    ")\n",
    "\n",
    "# For Jupyter async support\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Usage: Mock Mode\n",
    "\n",
    "Let's start with mock interfaces that don't require Lean 4 or a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mock interfaces\n",
    "lean = MockLeanInterface()\n",
    "inference = MockInferenceClient()\n",
    "\n",
    "# Create a rollout generator\n",
    "generator = ProofRolloutGenerator(\n",
    "    inference_client=inference,\n",
    "    lean_interface=lean,\n",
    "    max_steps=5,\n",
    ")\n",
    "\n",
    "# Generate a rollout\n",
    "trajectory = asyncio.run(generator.generate_rollout(\n",
    "    theorem_name=\"add_zero\",\n",
    "    theorem_statement=\"theorem add_zero (n : Nat) : n + 0 = n\",\n",
    "))\n",
    "\n",
    "print(f\"Reward: {trajectory.reward}\")\n",
    "print(f\"Steps: {trajectory.num_steps}\")\n",
    "print(f\"Complete: {trajectory.is_complete}\")\n",
    "print(\"\\nTactics:\")\n",
    "print(trajectory.get_tactics_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Lean 4 Integration\n",
    "\n",
    "Now let's use the actual Lean 4 interface (requires Lean 4 installed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Lean is available\n",
    "try:\n",
    "    lean_real = LeanInterface(lean_cmd=\"lake\", timeout=10.0)\n",
    "    print(\"Lean 4 interface initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize Lean: {e}\")\n",
    "    lean_real = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if lean_real:\n",
    "    from lean_grpo.lean_interface import LeanProofState\n",
    "    \n",
    "    # Create a proof state\n",
    "    state = LeanProofState(\n",
    "        theorem_name=\"test\",\n",
    "        theorem_statement=\"(n : Nat) : n = n\",\n",
    "        imports=[\"Mathlib\"],\n",
    "    )\n",
    "    \n",
    "    # Execute a tactic\n",
    "    result = asyncio.run(lean_real.execute_tactic(state, \"intro n\"))\n",
    "    \n",
    "    print(f\"Status: {result.status}\")\n",
    "    print(f\"Goals remaining: {result.num_goals}\")\n",
    "    print(f\"Is valid: {result.is_valid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reward Calculation\n",
    "\n",
    "Calculate rewards for proof trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lean_grpo.reward import LeanRewardCalculator\n",
    "\n",
    "# Create reward calculator\n",
    "calculator = LeanRewardCalculator(\n",
    "    lean_interface=lean,\n",
    "    config=REWARD_CONFIG_SHAPED,\n",
    ")\n",
    "\n",
    "# Calculate reward for our trajectory\n",
    "reward, metrics = asyncio.run(calculator.calculate_reward(trajectory))\n",
    "\n",
    "print(f\"Reward: {reward:.3f}\")\n",
    "print(\"\\nMetrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Setup\n",
    "\n",
    "Configure and set up the GRPO trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "config = LeanGRPOConfig(\n",
    "    base_model=\"Qwen/Qwen2.5-0.5B-Instruct\",  # Small model for testing\n",
    "    lora_rank=8,\n",
    "    num_generations=4,\n",
    "    learning_rate=5e-6,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    max_proof_steps=10,\n",
    "    reward_config=REWARD_CONFIG_SHAPED,\n",
    "    output_dir=\"outputs/tutorial\",\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Base model: {config.base_model}\")\n",
    "print(f\"  LoRA rank: {config.lora_rank}\")\n",
    "print(f\"  Num generations: {config.num_generations}\")\n",
    "print(f\"  Learning rate: {config.learning_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Dataset\n",
    "\n",
    "Create a dataset of theorems to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample theorems\n",
    "theorems = [\n",
    "    {\n",
    "        \"name\": \"refl\",\n",
    "        \"statement\": \"theorem refl (n : Nat) : n = n\",\n",
    "        \"imports\": [\"Mathlib\"],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"succ_zero\",\n",
    "        \"statement\": \"theorem succ_zero : Nat.succ 0 = 1\",\n",
    "        \"imports\": [\"Mathlib\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(theorems)} theorems\")\n",
    "for thm in theorems:\n",
    "    print(f\"  - {thm['name']}: {thm['statement']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Trainer\n",
    "\n",
    "Set up the trainer (requires GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"WARNING: No GPU available. Training will not work.\")\n",
    "else:\n",
    "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = LeanGRPOTrainer(config, lean, inference)\n",
    "    trainer.setup()\n",
    "    \n",
    "    print(\"\\nTrainer initialized!\")\n",
    "    print(f\"Model: {trainer.model.__class__.__name__}\")\n",
    "    print(f\"Tokenizer: {trainer.tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dataset = trainer.prepare_dataset(theorems, trainer.tokenizer)\n",
    "    print(f\"\\nDataset prepared with {len(dataset)} examples\")\n",
    "    \n",
    "    # Show an example\n",
    "    example = dataset[0]\n",
    "    print(\"\\nExample prompt:\")\n",
    "    for msg in example['prompt']:\n",
    "        print(f\"  {msg['role']}: {msg['content'][:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train\n",
    "\n",
    "Run training (this will take a while on GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available() and len(dataset) > 0:\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train(dataset)\n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "    # Save model\n",
    "    trainer.save_model()\n",
    "    print(f\"Model saved to {config.output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Proofs\n",
    "\n",
    "Use the trained model to generate proofs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Generate a proof\n",
    "    theorem = \"theorem add_one (n : Nat) : n + 1 = Nat.succ n\"\n",
    "    \n",
    "    print(f\"Theorem: {theorem}\")\n",
    "    print(\"\\nGenerating proof...\")\n",
    "    \n",
    "    proof = trainer.generate_proof(theorem, temperature=0.7)\n",
    "    \n",
    "    print(\"\\nGenerated proof:\")\n",
    "    print(proof)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced: Custom Reward Function\n",
    "\n",
    "Create a custom reward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lean_grpo.reward import RewardConfig\n",
    "from lean_grpo.trajectory import ProofTrajectory\n",
    "\n",
    "def prefer_short_proofs(trajectory: ProofTrajectory) -> float:\n",
    "    \"\"\"Reward shorter proofs.\"\"\"\n",
    "    if trajectory.is_complete:\n",
    "        # Bonus for short proofs\n",
    "        return max(0, 0.5 - trajectory.num_steps * 0.05)\n",
    "    return 0.0\n",
    "\n",
    "# Create custom config\n",
    "custom_config = RewardConfig(\n",
    "    completion_reward=1.0,\n",
    "    custom_rewards=[prefer_short_proofs],\n", "    use_lean_validation=True,\n",
    ")\n",
    "\n",
    "# Use in trainer\n",
    "# trainer = LeanGRPOTrainer(config, lean, inference, reward_config=custom_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial covered:\n",
    "\n",
    "1. Basic usage with mock interfaces\n",
    "2. Lean 4 integration\n",
    "3. Reward calculation\n",
    "4. Training setup\n",
    "5. Dataset preparation\n",
    "6. Model training\n",
    "7. Proof generation\n",
    "8. Custom reward functions\n",
    "\n",
    "For more details, see the documentation and examples in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
