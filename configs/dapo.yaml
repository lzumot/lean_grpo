# DAPO (Decoupled Advantage Policy Optimization) Configuration
# Best for: Sparse rewards, diverse problem difficulties

base_model: "Qwen/Qwen2.5-7B-Instruct"
lora_rank: 8

# Algorithm
algorithm: "dapo"
algorithm_config:
  # Group configuration
  group_size: 8
  
  # Population-based normalization
  use_population_norm: true
  population_size: 100
  advantage_ema_decay: 0.99
  
  # Asymmetric loss
  use_asymmetric_loss: true
  positive_advantage_scale: 1.0
  negative_advantage_scale: 0.8
  
  # Clipping
  epsilon: 0.2
  epsilon_high: 0.2
  ratio_clip_high: 1.5
  ratio_clip_low: 0.5
  
  # Sparse reward handling
  use_reward_shaping: true
  sparse_reward_threshold: 0.9
  sparse_reward_bonus: 0.1
  
  # Advantage clipping
  advantage_clip: 5.0

# Training
learning_rate: 5e-6
num_train_epochs: 3
per_device_train_batch_size: 4
num_generations: 8

# Reward
reward_type: "shaped"

# Output
output_dir: "outputs/dapo"
report_to: "wandb"
